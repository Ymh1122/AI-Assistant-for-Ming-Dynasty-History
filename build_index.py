# 1. é…ç½®è·¯å¾„
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'#ï¼ï¼ï¼ï¼å…³æ¢¯å­è¿è¡Œæ›´å¿«ï¼ï¼ï¼
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
import glob
import pickle
import numpy as np
import re
from sentence_transformers import SentenceTransformer

# --- æ ¸å¿ƒä¿®æ”¹å¼€å§‹ ---
# 1. è·å–å½“å‰è„šæœ¬(build_index.py)æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
current_script_path = os.path.dirname(os.path.abspath(__file__))

# 2. æ‹¼æ¥å‡ºæ•°æ®æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„
# è¿™æ ·æ— è®ºä½ åœ¨ç»ˆç«¯å“ªä¸ªç›®å½•ä¸‹è¿è¡Œï¼ŒPython éƒ½èƒ½ç²¾å‡†æ‰¾åˆ°æ¡Œé¢ä¸Šè¿™ä¸ªæ–‡ä»¶å¤¹
DATA_FOLDER = os.path.join(current_script_path, 'ming_dynasty_cn')

print(f"ğŸ“ é”å®šæ•°æ®è·¯å¾„: {DATA_FOLDER}")
# --- æ ¸å¿ƒä¿®æ”¹ç»“æŸ ---

def classify_entry(name):
    """
    æ ¹æ®æ–‡ä»¶åç®€å•æ¨æ–­æ¡ç›®ç±»å‹
    """
    if any(k in name for k in ['å²', 'ä¹¦', 'å…¸', 'å¾‹', 'è®°', 'è€ƒ', 'å½•']):
        return 'å…¸ç±'
    if any(k in name for k in ['å˜', 'æˆ˜', 'å½¹', 'æ¡ˆ', 'äº‰', 'ä¹±', 'æ³•', 'åˆ¶', 'é¥·', 'è¾¹', 'å«']):
        return 'äº‹ä»¶/åˆ¶åº¦'
    # é»˜è®¤è§†ä¸ºäººç‰©
    return 'äººç‰©'

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ä¸­çš„ URL å’Œå…¶ä»–æ— å…³å­—ç¬¦"""
    # å»é™¤ URL
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def read_and_chunk_files(folder_path, chunk_size=150):
    """
    è¯»å–æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰txtï¼Œå¹¶æŒ‰é•¿åº¦åˆ‡åˆ†æˆå°æ®µ
    chunk_size: æ¯æ®µå¤§çº¦å¤šå°‘å­—
    """
    all_chunks = []
    
    # æŸ¥æ‰¾æ‰€æœ‰ .txt æ–‡ä»¶
    txt_files = glob.glob(os.path.join(folder_path, "*.txt"))
    
    if not txt_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ '{folder_path}' ä¸‹æ²¡æ‰¾åˆ° .txt æ–‡ä»¶ï¼è¯·æ£€æŸ¥æ–‡ä»¶å¤¹åå­—ã€‚")
        return []

    print(f"ğŸ“‚ å‘ç° {len(txt_files)} ä¸ªå†å²æ¡ç›®æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    for file_path in txt_files:
        # ä»æ–‡ä»¶åæå–æ¡ç›®å
        file_name = os.path.basename(file_path)
        entry_name = file_name.replace('.txt', '')
        category = classify_entry(entry_name)
        
        try:
            # å°è¯• UTF-8 è¯»å–ï¼Œå¦‚æœæŠ¥é”™å°è¯• GBK (é˜²æ­¢ Windows ç¼–ç é—®é¢˜)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk', errors='ignore') as f:
                content = f.read()

        # æ¸…ç†æ–‡æœ¬
        content = clean_text(content)

        # --- åˆ‡ç‰‡é€»è¾‘ (Chunking) ---
        # ç®€å•ç²—æš´ä½†æœ‰æ•ˆï¼šæŒ‰å¥å·æ‹†åˆ†ï¼Œç„¶åæ‹¼å‡‘æˆ chunk_size å¤§å°çš„å—
        sentences = content.replace('\n', '').split('ã€‚')
        
        current_chunk = ""
        for sent in sentences:
            if not sent.strip(): continue
            
            current_chunk += sent + "ã€‚"
            
            # å¦‚æœå½“å‰å—å¤Ÿé•¿äº†ï¼Œå°±å­˜èµ·æ¥ï¼Œå¹¶å¼€å¯æ–°çš„ä¸€å—
            if len(current_chunk) >= chunk_size:
                all_chunks.append({
                    "id": f"{entry_name}_{len(all_chunks)}",
                    "name": entry_name,
                    "category": category, # æ–°å¢åˆ†ç±»å­—æ®µ
                    "text": current_chunk
                })
                current_chunk = "" # é‡ç½®
        
        # å¤„ç†æœ€åå‰©ä½™çš„ä¸€ç‚¹ç‚¹æ–‡æœ¬
        if current_chunk:
            all_chunks.append({
                "id": f"{entry_name}_last",
                "name": entry_name,
                "category": category,
                "text": current_chunk
            })
            
    return all_chunks

def create_embeddings():
    # 1. è¯»å–å¹¶åˆ‡åˆ†æ•°æ®
    wiki_data = read_and_chunk_files(DATA_FOLDER)
    
    if not wiki_data:
        return

    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼å…±åˆ‡åˆ†ä¸º {len(wiki_data)} ä¸ªæ–‡æœ¬ç‰‡æ®µã€‚")
    print("â³ æ­£åœ¨åŠ è½½ BGE æ¨¡å‹ (ç¬¬ä¸€æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½)...")
    
    model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
    
    print("ğŸš€ æ­£åœ¨ç”Ÿæˆå‘é‡ (è¿™å¯èƒ½éœ€è¦å‡ åç§’)...")
    texts = [item["text"] for item in wiki_data]
    
    # normalize_embeddings=True å¯¹è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦éå¸¸é‡è¦
    embeddings = model.encode(texts, normalize_embeddings=True)
    
    print(f"ğŸ“Š å‘é‡ç”Ÿæˆå®Œæ¯•ã€‚ç»´åº¦: {embeddings.shape}")

    # ä¿å­˜åˆ°æœ¬åœ°
    output_file = 'ming_vectors.pkl'
    with open(output_file, 'wb') as f:
        pickle.dump({'data': wiki_data, 'embeddings': embeddings}, f)
    
    print(f"ğŸ’¾ æ•°æ®åº“å·²ä¿å­˜ä¸º: {output_file}")

if __name__ == "__main__":
    create_embeddings()
