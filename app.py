import streamlit as st
import os
import pickle
import numpy as np
import pandas as pd
import requests
import json
import zhconv
import plotly.express as px
from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA

# --- 0. åŸºç¡€é…ç½® ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_FILE = os.path.join(BASE_DIR, 'ming_vectors.pkl')

st.set_page_config(page_title="æ˜åŸŸ Â· ä¼ªå²ç”Ÿæˆç³»ç»Ÿ", layout="wide", page_icon="ğŸ‰")

# --- æ ¸å¿ƒæ¶æ„ç±»å®šä¹‰ ---

class HistoryEmbeddingLayer:
    """
    ç¬¬1å±‚ï¼šå†å²äº‹å®åµŒå…¥å±‚
    åŠŸèƒ½ï¼šåŠ è½½â€œæ˜ä»£å†å²çŸ¥è¯†å›¾è°±åµŒå…¥ç©ºé—´â€ï¼Œæä¾›å‘é‡åŒ–å’Œæ£€ç´¢èƒ½åŠ›ã€‚
    """
    def __init__(self, vector_file):
        self.vector_file = vector_file
        self.model = None
        self.db_data = None
        self.db_embeddings = None
        self._load_resources()

    def _load_resources(self):
        # ä½¿ç”¨ st.cache_resource é¿å…é‡å¤åŠ è½½
        if 'model' not in st.session_state:
            st.session_state.model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
        self.model = st.session_state.model

        if not os.path.exists(self.vector_file):
            st.error(f"âŒ æ‰¾ä¸åˆ° {self.vector_file}ï¼è¯·å…ˆè¿è¡Œ build_index.py")
            return

        if 'db_data' not in st.session_state:
            with open(self.vector_file, 'rb') as f:
                data = pickle.load(f)
                st.session_state.db_data = data['data']
                st.session_state.db_embeddings = data['embeddings']
        
        self.db_data = st.session_state.db_data
        self.db_embeddings = st.session_state.db_embeddings

    def encode(self, text):
        return self.model.encode([text], normalize_embeddings=True)

    def search(self, query_vec, top_k=3):
        if self.db_embeddings is None: return []
        scores = np.dot(self.db_embeddings, query_vec.T).flatten()
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                "score": scores[idx],
                "data": self.db_data[idx],
                "vector": self.db_embeddings[idx]
            })
        return results

class ContextAlignmentLayer:
    """
    ç¬¬2å±‚ï¼šåˆ¶åº¦-è¯­å¢ƒå¯¹é½å±‚
    åŠŸèƒ½ï¼šç¡®ä¿ç”Ÿæˆå†…å®¹ç¬¦åˆæ˜ä»£åˆ¶åº¦é€»è¾‘ï¼ˆå¦‚å«æ‰€ã€é‡Œç”²ã€ç§‘ä¸¾ã€å‚å«ï¼‰ã€‚
    """
    def __init__(self):
        self.keywords = [
            "å«æ‰€", "é”¦è¡£å«", "ä¸œå‚", "è¥¿å‚", "å†…é˜", "ç§‘ä¸¾", "å…­éƒ¨", 
            "å·¡æŠš", "æ€»ç£", "é‡Œç”²", "é»„å†Œ", "é±¼é³å›¾å†Œ", "æµ·ç¦", "æœè´¡",
            "å¸ç¤¼ç›‘", "ç¿°æ—é™¢", "å›½å­ç›‘", "å¸ƒæ”¿ä½¿", "æŒ‰å¯Ÿä½¿", "éƒ½æŒ‡æŒ¥ä½¿"
        ]

    def validate(self, text):
        """ç®€å•æ¨¡æ‹Ÿâ€œå¤šä»»åŠ¡å­¦ä¹ ï¼šåˆ¶åº¦åˆ†ç±»å¤´â€"""
        found_keywords = [kw for kw in self.keywords if kw in text]
        score = len(found_keywords) * 0.2  # ç®€å•çš„å¯å‘å¼æ‰“åˆ†
        return {
            "is_valid": len(found_keywords) > 0,
            "score": min(score, 1.0),
            "keywords": found_keywords
        }

class FictionDiffusionLayer:
    """
    ç¬¬3å±‚ï¼šåˆç†è™šæ„æ‰©æ•£å±‚
    åŠŸèƒ½ï¼šåœ¨å†å²è¯­ä¹‰é‚»åŸŸå†…è¿›è¡Œå—æ§å‘é‡æ’å€¼ï¼Œç”Ÿæˆâ€œæœªè®°è½½ä½†å¯èƒ½â€çš„äº‹ä»¶ç»†èŠ‚ã€‚
    """
    def __init__(self, embedding_layer):
        self.emb_layer = embedding_layer

    def interpolate_and_generate(self, fact_vec, query_vec, alpha=0.3):
        """
        Constrained Diffusion in Embedding Space (æ¨¡æ‹Ÿ)
        V_gen = (1 - alpha) * V_fact + alpha * V_query
        """
        # å‘é‡æ’å€¼
        # alpha è¶Šå¤§ï¼Œè¶Šåå‘ç”¨æˆ·çš„â€œè™šæ„/æŸ¥è¯¢â€ï¼›alpha è¶Šå°ï¼Œè¶Šåå‘â€œå²å®â€
        gen_vec = (1 - alpha) * fact_vec + alpha * query_vec
        
        # å½’ä¸€åŒ–ï¼ˆä¿æŒåœ¨å•ä½çƒé¢ä¸Šï¼Œç¬¦åˆ cosine similarity ç‰¹æ€§ï¼‰
        norm = np.linalg.norm(gen_vec)
        if norm > 0:
            gen_vec = gen_vec / norm
            
        # åœ¨ç©ºé—´ä¸­å¯»æ‰¾æœ€è¿‘çš„â€œæ½œåœ¨å²æ–™â€ä½œä¸ºç”Ÿæˆçš„åŸºåº•
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¯»æ‰¾çš„æ˜¯é™¤äº†åŸå§‹ fact ä¹‹å¤–æœ€è¿‘çš„ç‚¹ï¼Œä»£è¡¨â€œå¯èƒ½çš„å˜ä½“â€
        results = self.emb_layer.search(gen_vec, top_k=5)
        
        return gen_vec, results

# --- è¾…åŠ©å‡½æ•° (CBDB) ---
def get_cbdb_bio(name_cn):
    """ä»å“ˆä½› CBDB è·å–ç»“æ„åŒ–æ•°æ®"""
    try:
        name_trad = zhconv.convert(name_cn, 'zh-hant')
        url = "https://cbdb.fas.harvard.edu/cbdbapi/person.php"
        params = {"name": name_trad, "o": "json"}
        resp = requests.get(url, params=params, timeout=3)
        data = json.loads(resp.text)
        if 'Package' in data: data = data['Package']
        if 'PersonAuthority' in data: data = data['PersonAuthority']
        if 'PersonInfo' in data: data = data['PersonInfo']
        if 'Person' in data: data = data['Person']
        
        if isinstance(data, dict): target = data
        elif isinstance(data, list): target = data[0]
        else: return None
        
        basic = target.get('BasicInfo', {})
        return {
            "name": basic.get('ChName', name_cn),
            "birth": basic.get('YearBirth', '?'),
            "death": basic.get('YearDeath', '?'),
            "dynasty": basic.get('Dynasty', 'æ˜'),
            "native": basic.get('IndexAddr', 'æœªçŸ¥'),
            "id": basic.get('PersonId', 'N/A')
        }
    except:
        return None

# --- UI é€»è¾‘ ---

def main():
    # åˆå§‹åŒ–å„å±‚
    layer1 = HistoryEmbeddingLayer(VECTOR_FILE)
    layer2 = ContextAlignmentLayer()
    layer3 = FictionDiffusionLayer(layer1)

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.title("ğŸ‰ æ˜åŸŸ MingYu")
        st.caption("åŸºäºå†å²è¯­ä¹‰åµŒå…¥çš„åˆç†ä¼ªå²ç”Ÿæˆç³»ç»Ÿ")
        st.divider()
        
        st.header("âš™ï¸ ç³»ç»Ÿå‚æ•° (System Params)")
        alpha = st.slider("è™šæ„æ‰©æ•£ç³»æ•° (Alpha)", 0.0, 1.0, 0.3, help="0=å®Œå…¨å²å®, 1=å®Œå…¨è™šæ„")
        threshold = st.slider("åˆç†æ€§é˜ˆå€¼ (Credibility)", 0.0, 1.0, 0.4, help="è¿‡æ»¤æ‰è¯­ä¹‰è·ç¦»è¿‡è¿œçš„ç»“æœ")
        
        st.info("ğŸ’¡ **æ“ä½œæŒ‡å—**ï¼š\nè¾“å…¥ä¸€ä¸ªâ€œå‡å¦‚â€çš„å†å²æƒ…å¢ƒï¼Œç³»ç»Ÿå°†åœ¨æ˜ä»£è¯­ä¹‰æµå½¢ä¸­å¯»æ‰¾æœ€åˆç†çš„â€œä¼ªå²â€è½ç‚¹ã€‚")

    # ä¸»ç•Œé¢
    st.title("ã€Šæ˜åŸŸã€‹ï¼šåˆç†ä¼ªå²ç”Ÿæˆæ§åˆ¶å°")
    st.markdown("""
    > **æ ¸å¿ƒç†å¿µ**ï¼šåœ¨æ˜ä»£å†å²çš„è¯­ä¹‰æµå½¢ä¸Šï¼Œè¿›è¡Œæœ‰ç•Œçš„å†å²æƒ³è±¡åŠ›æ¢ç´¢ã€‚
    """)
    
    query = st.text_input("ğŸ“ è¾“å…¥å†å²å‡è®¾ / æ¢ç´¢èŠ‚ç‚¹", "å‡å¦‚å¼ å±…æ­£æ”¯æŒä¸‡å†çš‡å¸å½»åº•æ¸…ç®—å†¯ä¿")
    
    if st.button("å¯åŠ¨ç”Ÿæˆå¼•æ“", type="primary"):
        if not layer1.db_data:
            st.error("æ•°æ®æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥ build_index.py æ˜¯å¦è¿è¡Œã€‚")
            st.stop()
            
        with st.spinner("æ­£åœ¨éå†å†å²è¯­ä¹‰æµå½¢..."):
            # 1. ç¼–ç ç”¨æˆ·è¾“å…¥ (Layer 1)
            query_vec = layer1.encode(query)
            
            # 2. æ£€ç´¢æœ€è¿‘çš„å†å²äº‹å® (Layer 1)
            # è¿™æ˜¯â€œé”šç‚¹â€ï¼Œç¡®ä¿è™šæ„ä¸è„±ç¦»å†å²åŸºåº•
            fact_results = layer1.search(query_vec, top_k=1)
            fact_item = fact_results[0]
            fact_vec = fact_item['vector']
            
            # 3. å‘é‡æ’å€¼ä¸æ‰©æ•£ (Layer 3)
            gen_vec, nearby_results = layer3.interpolate_and_generate(fact_vec, query_vec, alpha)
            
            # 4. åˆ¶åº¦æ ¡éªŒ (Layer 2)
            # å¯¹ç”Ÿæˆç»“æœï¼ˆè¿™é‡Œç”¨æœ€è¿‘é‚»è¿‘ä¼¼ï¼‰è¿›è¡Œæ ¡éªŒ
            best_match = nearby_results[0] # æœ€æ¥è¿‘æ’å€¼ç‚¹çš„æ–‡æœ¬
            validation = layer2.validate(best_match['data']['text'])
            
        # --- ç»“æœå±•ç¤º ---
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“ å†å²é”šç‚¹ (Fact Anchor)")
            st.success(f"**{fact_item['data']['name']}** (ç›¸ä¼¼åº¦: {fact_item['score']:.4f})")
            st.markdown(f"_{fact_item['data']['text']}_")
            
            st.divider()
            
            st.subheader("ğŸ² ç”Ÿæˆçš„åˆç†ä¼ªå² (Generated Pseudo-History)")
            st.caption(f"åŸºäºæ’å€¼å‘é‡ (Alpha={alpha}) åœ¨è¯­ä¹‰ç©ºé—´ä¸­å¬å›çš„æœ€è¿‘é‚»çŠ¶æ€")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„â€œä¼ªå²â€ç‰‡æ®µï¼ˆå…¶å®æ˜¯è¯­ä¹‰ç©ºé—´ä¸­ä»‹äºäº‹å®å’Œè™šæ„ä¹‹é—´çš„çœŸå®ç‰‡æ®µï¼Œä½œä¸ºæ¨¡æ‹Ÿï¼‰
            gen_text = best_match['data']['text']
            gen_name = best_match['data']['name']
            gen_score = best_match['score'] # è¿™é‡Œæ˜¯ä¸æ’å€¼å‘é‡çš„è·ç¦»
            
            st.info(f"**ç›¸å…³äººç‰©ï¼š{gen_name}**")
            st.write(gen_text)
            
            # åˆ¶åº¦æ ¡éªŒç»“æœ
            st.markdown("#### ğŸ›¡ï¸ åˆ¶åº¦-è¯­å¢ƒå¯¹é½å±‚æ ¡éªŒ")
            if validation['is_valid']:
                st.success(f"âœ… é€šè¿‡æ ¡éªŒ (Score: {validation['score']:.2f})")
                st.markdown(f"**è¯†åˆ«åˆ°çš„åˆ¶åº¦å…³é”®è¯**ï¼š`{', '.join(validation['keywords'])}`")
            else:
                st.warning("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°å…¸å‹çš„æ˜ä»£åˆ¶åº¦ç‰¹å¾ï¼Œç”Ÿæˆå†…å®¹å¯èƒ½åç¦»æ—¶ä»£è¯­å¢ƒã€‚")
                
        with col2:
            st.subheader("ğŸŒŒ è¯­ä¹‰æµå½¢å¯è§†åŒ–")
            
            # å‡†å¤‡ç»˜å›¾æ•°æ®
            # 1. äº‹å®ç‚¹
            # 2. ç”¨æˆ·æŸ¥è¯¢ç‚¹
            # 3. ç”Ÿæˆç‚¹ (æ’å€¼ç‚¹)
            # 4. èƒŒæ™¯ç‚¹ (éšæœºå–ä¸€äº›)
            
            subset_indices = list(range(min(len(layer1.db_data), 50)))
            subset_vecs = layer1.db_embeddings[subset_indices]
            subset_names = [layer1.db_data[i]['name'] for i in subset_indices]
            
            # é™ç»´
            all_vecs = np.vstack([subset_vecs, fact_vec, query_vec, gen_vec])
            pca = PCA(n_components=2)
            all_coords = pca.fit_transform(all_vecs)
            
            # èƒŒæ™¯æ•°æ®
            bg_len = len(subset_vecs)
            df_bg = pd.DataFrame({
                'x': all_coords[:bg_len, 0],
                'y': all_coords[:bg_len, 1],
                'label': subset_names,
                'type': ['History Background'] * bg_len
            })
            
            # ç‰¹æ®Šç‚¹
            df_special = pd.DataFrame({
                'x': [all_coords[bg_len, 0], all_coords[bg_len+1, 0], all_coords[bg_len+2, 0]],
                'y': [all_coords[bg_len, 1], all_coords[bg_len+1, 1], all_coords[bg_len+2, 1]],
                'label': ['å†å²é”šç‚¹ (Fact)', 'ç”¨æˆ·å‡è®¾ (Query)', 'ç”Ÿæˆä¼ªå² (Generated)'],
                'type': ['Anchor', 'Query', 'Generated']
            })
            
            final_df = pd.concat([df_bg, df_special])
            
            fig = px.scatter(final_df, x='x', y='y', color='type', hover_data=['label'],
                             symbol='type', size_max=15, title="å†å²è¯­ä¹‰æ‹“æ‰‘ç©ºé—´")
            
            fig.update_traces(marker=dict(size=12))
            st.plotly_chart(fig, use_container_width=True)
            
            st.caption("""
            **å›¾ä¾‹è¯´æ˜**ï¼š
            - **Anchor**: çœŸå®å†å²ä¸­ä¸å‡è®¾æœ€æ¥è¿‘çš„äº‹ä»¶ã€‚
            - **Query**: ä½ çš„å‡è®¾åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„ä½ç½®ã€‚
            - **Generated**: ç³»ç»Ÿæ ¹æ® Alpha æ’å€¼è®¡ç®—å‡ºçš„â€œä¼ªå²â€è½ç‚¹ã€‚
            """)
            
            # CBDB è¡¥å……ä¿¡æ¯
            if validation['is_valid'] and gen_name != 'æœªçŸ¥':
                 st.divider()
                 st.markdown(f"**ğŸ“œ {gen_name} çš„çœŸå®å±¥å† (CBDB)**")
                 bio = get_cbdb_bio(gen_name)
                 if bio:
                     st.json(bio)
                 else:
                     st.write("æ— è¯¦ç»†è®°å½•")

if __name__ == "__main__":
    main()
