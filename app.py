import os
# å¿…é¡»åœ¨å¯¼å…¥ sentence_transformers ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™é•œåƒæºå¯èƒ½ä¸ç”Ÿæ•ˆ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# æŠ‘åˆ¶ TensorFlow æ—¥å¿—
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import dashscope
from sklearn.decomposition import PCA

# Import core logic
from core_logic import (
    HistoryEmbeddingLayer,
    ContextAlignmentLayer,
    FictionDiffusionLayer,
    QwenGenerationLayer,
    ContentAuditor,
    ExternalKnowledgeLayer
)

# --- 0. åŸºç¡€é…ç½® ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_FILE = os.path.join(BASE_DIR, 'ming_vectors.pkl')

# åŠ è½½ API Key
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

api_key = os.getenv('DASHSCOPE_API_KEY')
if api_key:
    dashscope.api_key = api_key
else:
    st.warning("âš ï¸ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨ Qwen ç”Ÿæˆæ–‡æœ¬ã€‚")

st.set_page_config(page_title="æ˜åŸŸ Â· ä¼ªå²ç”Ÿæˆç³»ç»Ÿ", layout="wide", page_icon="ğŸ‰")

# --- UI é€»è¾‘ ---

def main():
    # åˆå§‹åŒ–å„å±‚
    layer1 = HistoryEmbeddingLayer(VECTOR_FILE)
    layer2 = ContextAlignmentLayer()
    layer3 = FictionDiffusionLayer(layer1)
    layer4 = QwenGenerationLayer()
    auditor = ContentAuditor()

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.title("ğŸ‰ æ˜åŸŸ MingYu")
        st.caption("åŸºäºå†å²è¯­ä¹‰åµŒå…¥çš„åˆç†ä¼ªå²ç”Ÿæˆç³»ç»Ÿ")
        st.divider()
        
        st.header("âš™ï¸ ç³»ç»Ÿå‚æ•° (System Params)")
        alpha = st.slider("è™šæ„æ‰©æ•£ç³»æ•° (Alpha)", 0.0, 1.0, 0.3, help="0=å®Œå…¨å²å®, 1=å®Œå…¨è™šæ„")
        threshold = st.slider("åˆç†æ€§é˜ˆå€¼ (Credibility)", 0.0, 1.0, 0.4, help="è¿‡æ»¤æ‰è¯­ä¹‰è·ç¦»è¿‡è¿œçš„ç»“æœ")
        
        st.info("ğŸ’¡ **æ“ä½œæŒ‡å—**ï¼š\nè¾“å…¥ä¸€ä¸ªâ€œå‡å¦‚â€çš„å†å²æƒ…å¢ƒï¼Œç³»ç»Ÿå°†åœ¨æ˜ä»£è¯­ä¹‰æµå½¢ä¸­å¯»æ‰¾æœ€åˆç†çš„â€œä¼ªå²â€è½ç‚¹ã€‚")

    # ä¸»ç•Œé¢
    st.title("ã€Šæ˜åŸŸã€‹ï¼šåˆç†ä¼ªå²ç”Ÿæˆæ§åˆ¶å°")
    st.markdown("""
    > **æ ¸å¿ƒç†å¿µ**ï¼šåœ¨æ˜ä»£å†å²çš„è¯­ä¹‰æµå½¢ä¸Šï¼Œè¿›è¡Œæœ‰ç•Œçš„å†å²æƒ³è±¡åŠ›æ¢ç´¢ã€‚
    """)
    
    query = st.text_input("ğŸ“ è¾“å…¥å†å²å‡è®¾ / æ¢ç´¢èŠ‚ç‚¹", "å‡å¦‚å¼ å±…æ­£æ”¯æŒä¸‡å†çš‡å¸å½»åº•æ¸…ç®—å†¯ä¿")
    
    if st.button("å¯åŠ¨ç”Ÿæˆå¼•æ“", type="primary"):
        if not layer1.db_data:
            st.error("æ•°æ®æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥ build_index.py æ˜¯å¦è¿è¡Œã€‚")
            st.stop()
            
        with st.spinner("æ­£åœ¨éå†å†å²è¯­ä¹‰æµå½¢å¹¶ç”Ÿæˆä¼ªå²..."):
            # 1. ç¼–ç ç”¨æˆ·è¾“å…¥ (Layer 1)
            query_vec = layer1.encode(query)
            
            # 2. æ£€ç´¢æœ€è¿‘çš„å†å²äº‹å® (Layer 1)
            # è¿™æ˜¯â€œé”šç‚¹â€ï¼Œç¡®ä¿è™šæ„ä¸è„±ç¦»å†å²åŸºåº•
            fact_results = layer1.search(query_vec, top_k=1)
            fact_item = fact_results[0]
            fact_vec = fact_item['vector']
            
            # 3. å‘é‡æ’å€¼ä¸æ‰©æ•£ (Layer 3)
            # ä¼ å…¥ exclude_idï¼Œç¡®ä¿ä¸è¿”å›å²å®æœ¬èº«
            gen_vec, nearby_results = layer3.interpolate_and_generate(
                fact_vec, 
                query_vec, 
                alpha, 
                exclude_id=fact_item['data']['id']
            )
            
            # 4. åˆ¶åº¦æ ¡éªŒ (Layer 2)
            # å¯¹ç”Ÿæˆç»“æœï¼ˆè¿™é‡Œç”¨æœ€è¿‘é‚»è¿‘ä¼¼ï¼‰è¿›è¡Œæ ¡éªŒ
            best_match = nearby_results[0] # æœ€æ¥è¿‘æ’å€¼ç‚¹çš„æ–‡æœ¬
            validation = layer2.validate(best_match['data']['text'])
            
            # 5. å¤§æ¨¡å‹ç”Ÿæˆ (Layer 4 - NEW)
            # æå– context æ–‡æœ¬åˆ—è¡¨
            nearby_texts = [r['data']['text'] for r in nearby_results]
            generated_pseudo_history = layer4.generate(
                query, 
                fact_item['data']['text'], 
                nearby_texts, 
                alpha
            )
            
            # 6. åŒé‡å®¡æ ¸ (Auditor)
            # å®¡æ ¸çš„æ˜¯å¤§æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè€Œä¸æ˜¯æ£€ç´¢åˆ°çš„æ–‡æœ¬
            audit_result = auditor.audit(query, generated_pseudo_history)
            
        # --- ç»“æœå±•ç¤º ---
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader(" å†å²é”šç‚¹ (Fact Anchor)")
            st.success(f"**{fact_item['data']['name']}** (ç›¸ä¼¼åº¦: {fact_item['score']:.4f})")
            st.markdown(f"_{fact_item['data']['text']}_")
            
            st.divider()
            
            st.subheader(" ç”Ÿæˆçš„åˆç†ä¼ªå² (Qwen Generated Pseudo-History)")
            st.caption(f"åŸºäºæ’å€¼å‘é‡ (Alpha={alpha}) + Qwen-Plus ç”Ÿæˆ")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„â€œä¼ªå²â€
            st.markdown(generated_pseudo_history)
            
            # åˆ¶åº¦æ ¡éªŒç»“æœ
            st.markdown("####  Layer 2: åˆ¶åº¦-è¯­å¢ƒå¯¹é½æ ¡éªŒ")
            # å¯¹å¤§æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œæ ¡éªŒ
            gen_validation = layer2.validate(generated_pseudo_history)
            
            if gen_validation['is_valid']:
                st.success(f" é€šè¿‡æ ¡éªŒ (Score: {gen_validation['score']:.2f})")
                st.markdown(f"**è¯†åˆ«åˆ°çš„åˆ¶åº¦å…³é”®è¯**ï¼š`{', '.join(gen_validation['keywords'])}`")
            else:
                st.warning("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°å…¸å‹çš„æ˜ä»£åˆ¶åº¦ç‰¹å¾ï¼Œç”Ÿæˆå†…å®¹å¯èƒ½åç¦»æ—¶ä»£è¯­å¢ƒã€‚")
                
            # åŒé‡å®¡æ ¸ç»“æœ
            st.markdown("####  Double Review: å†…å®¹åˆè§„æ€§å®¡æ ¸")
            if audit_result['passed']:
                st.success(f"âœ… {audit_result['message']}")
            else:
                st.error(f"âŒ {audit_result['message']}")
                st.caption("å»ºè®®ï¼šè°ƒæ•´ Alpha å€¼æˆ–ç»†åŒ–æŒ‡ä»¤ä»¥åŒ¹é…å·²æœ‰å²æ–™åº“ã€‚")
                
        with col2:
            st.subheader(" è¯­ä¹‰æµå½¢å¯è§†åŒ–")
            
            # å‡†å¤‡ç»˜å›¾æ•°æ®
            # 1. äº‹å®ç‚¹
            # 2. ç”¨æˆ·æŸ¥è¯¢ç‚¹
            # 3. ç”Ÿæˆç‚¹ (æ’å€¼ç‚¹)
            # 4. èƒŒæ™¯ç‚¹ (éšæœºå–ä¸€äº›)
            
            subset_indices = list(range(min(len(layer1.db_data), 50)))
            subset_vecs = layer1.db_embeddings[subset_indices]
            subset_names = [layer1.db_data[i]['name'] for i in subset_indices]
            
            # é™ç»´
            all_vecs = np.vstack([subset_vecs, fact_vec, query_vec, gen_vec])
            pca = PCA(n_components=2)
            all_coords = pca.fit_transform(all_vecs)
            
            # èƒŒæ™¯æ•°æ®
            bg_len = len(subset_vecs)
            df_bg = pd.DataFrame({
                'x': all_coords[:bg_len, 0],
                'y': all_coords[:bg_len, 1],
                'label': subset_names,
                'type': ['History Background'] * bg_len
            })
            
            # ç‰¹æ®Šç‚¹
            df_special = pd.DataFrame({
                'x': [all_coords[bg_len, 0], all_coords[bg_len+1, 0], all_coords[bg_len+2, 0]],
                'y': [all_coords[bg_len, 1], all_coords[bg_len+1, 1], all_coords[bg_len+2, 1]],
                'label': ['å†å²é”šç‚¹ (Fact)', 'ç”¨æˆ·å‡è®¾ (Query)', 'ç”Ÿæˆä¼ªå² (Generated)'],
                'type': ['Anchor', 'Query', 'Generated']
            })
            
            final_df = pd.concat([df_bg, df_special])
            
            fig = px.scatter(final_df, x='x', y='y', color='type', hover_data=['label'],
                             symbol='type', size_max=15, title="å†å²è¯­ä¹‰æ‹“æ‰‘ç©ºé—´")
            
            fig.update_traces(marker=dict(size=12))
            st.plotly_chart(fig, use_container_width=True)
            
            st.caption("""
            **å›¾ä¾‹è¯´æ˜**ï¼š
            - **Anchor**: çœŸå®å†å²ä¸­ä¸å‡è®¾æœ€æ¥è¿‘çš„äº‹ä»¶ã€‚
            - **Query**: ä½ çš„å‡è®¾åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„ä½ç½®ã€‚
            - **Generated**: ç³»ç»Ÿæ ¹æ® Alpha æ’å€¼è®¡ç®—å‡ºçš„â€œä¼ªå²â€è½ç‚¹ã€‚
            """)
            
            # CBDB è¡¥å……ä¿¡æ¯
            # åªæœ‰å½“æ¡ç›®è¢«å½’ç±»ä¸ºâ€œäººç‰©â€æ—¶æ‰è°ƒç”¨ CBDBï¼Œé¿å…ç”¨äº‹ä»¶åå»æŸ¥äººåæ•°æ®åº“
            category = best_match['data'].get('category', 'äººç‰©') # å…¼å®¹æ—§æ•°æ®ï¼Œé»˜è®¤ä¸ºäººç‰©
            gen_name = best_match['data']['name']
            
            if validation['is_valid'] and gen_name != 'æœªçŸ¥' and category == 'äººç‰©':
                 st.divider()
                 st.markdown(f"** {gen_name} çš„çœŸå®å±¥å† (CBDB)**")
                 bio = ExternalKnowledgeLayer.get_cbdb_bio(gen_name)
                 if bio:
                     st.json(bio)
                 else:
                     st.write("æ— è¯¦ç»†è®°å½•")
            elif category != 'äººç‰©':
                st.divider()
                st.info(f"â„¹ å½“å‰æ¡ç›®ç±»åˆ«ä¸º **{category}**ï¼Œä¸å±•ç¤ºäººç‰©å±¥å†ã€‚")

if __name__ == "__main__":
    main()
