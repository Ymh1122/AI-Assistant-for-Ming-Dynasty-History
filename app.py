import streamlit as st
import os
import pickle
import numpy as np
import pandas as pd
import requests
import json
import zhconv
import plotly.express as px
from sentence_transformers import SentenceTransformer
from sklearn.decomposition import PCA

# --- 0. åŸºç¡€é…ç½® (è§£å†³ç½‘ç»œå’Œè·¯å¾„é—®é¢˜) ---
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' # ç¡®ä¿æ¨¡å‹åŠ è½½ä¸å¡é¡¿
#pip install streamlit pandas plotly scikit-learn#ï¼ï¼å…³æ¢¯å­è¿è¡Œï¼ï¼ï¼

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ° .pkl æ–‡ä»¶
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_FILE = os.path.join(BASE_DIR, 'ming_vectors.pkl')

st.set_page_config(page_title="æ˜å² Â· è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ", layout="wide", page_icon="ğŸ“œ")

# --- 1. æ ¸å¿ƒèµ„æºåŠ è½½ (å¸¦ç¼“å­˜ï¼Œåªè·‘ä¸€æ¬¡) ---
@st.cache_resource
def load_resources():
    st.toast("æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹å’Œå‘é‡åº“...", icon="â³")
    
    # åŠ è½½æ¨¡å‹
    model = SentenceTransformer('BAAI/bge-small-zh-v1.5')
    
    # åŠ è½½æ•°æ®
    if not os.path.exists(VECTOR_FILE):
        st.error(f"âŒ æ‰¾ä¸åˆ° {VECTOR_FILE}ï¼è¯·å…ˆè¿è¡Œ build_index.py")
        return None, None, None
        
    with open(VECTOR_FILE, 'rb') as f:
        data = pickle.load(f)
        
    return model, data['data'], data['embeddings']

model, db_data, db_embeddings = load_resources()

# --- 2. CBDB API å‡½æ•° (æˆ‘ä»¬ä¹‹å‰è°ƒè¯•å®Œç¾çš„ç‰ˆæœ¬) ---
def get_cbdb_bio(name_cn):
    """ä»å“ˆä½› CBDB è·å–ç»“æ„åŒ–æ•°æ®"""
    name_trad = zhconv.convert(name_cn, 'zh-hant')
    url = "https://cbdb.fas.harvard.edu/cbdbapi/person.php"
    params = {"name": name_trad, "o": "json"}
    
    try:
        resp = requests.get(url, params=params, timeout=5)
        data = json.loads(resp.text)
        
        # å‰¥æ´‹è‘±é€»è¾‘
        if 'Package' in data: data = data['Package']
        if 'PersonAuthority' in data: data = data['PersonAuthority']
        if 'PersonInfo' in data: data = data['PersonInfo']
        if 'Person' in data: data = data['Person']
        
        # å½’ä¸€åŒ–å¤„ç†
        if isinstance(data, dict): target = data
        elif isinstance(data, list): target = data[0]
        else: return None
        
        basic = target.get('BasicInfo', {})
        return {
            "name": basic.get('ChName', name_cn),
            "birth": basic.get('YearBirth', '?'),
            "death": basic.get('YearDeath', '?'),
            "dynasty": basic.get('Dynasty', 'æ˜'),
            "native": basic.get('IndexAddr', 'æœªçŸ¥'),
            "id": basic.get('PersonId', 'N/A')
        }
    except:
        return None

# --- 3. è¯­ä¹‰æœç´¢é€»è¾‘ ---
def semantic_search(query, top_k=3):
    # 1. é—®é¢˜è½¬å‘é‡
    query_vec = model.encode([query], normalize_embeddings=True)
    # 2. è®¡ç®—ç›¸ä¼¼åº¦
    scores = np.dot(db_embeddings, query_vec.T).flatten()
    # 3. æ’åº
    top_indices = np.argsort(scores)[::-1][:top_k]
    
    results = []
    for idx in top_indices:
        results.append({
            "score": scores[idx],
            "data": db_data[idx]
        })
    return results, query_vec

# --- 4. ç•Œé¢ UI å¸ƒå±€ ---

# æ ‡é¢˜æ 
st.title("ğŸ“œ æ˜å² AI è¯­ä¹‰æ£€ç´¢ç³»ç»Ÿ")
st.markdown("ç»“åˆ **NLP Embeddings** ä¸ **CBDB æ•°æ®åº“** çš„æ•°å­—äººæ–‡æ¢ç´¢é¡¹ç›®")
st.divider()

# ä¾§è¾¹æ ï¼šæœç´¢æ§åˆ¶
with st.sidebar:
    st.header("ğŸ” æ¢ç´¢é¢æ¿")
    user_query = st.text_input("è¾“å…¥ä½ çš„é—®é¢˜", "å¼ å±…æ­£å’Œæˆšç»§å…‰æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ")
    
    st.info("ğŸ’¡ è¯•ä¸€è¯•ï¼š\n1. è°æ˜¯æ˜æœå¼€å›½çš‡å¸ï¼Ÿ\n2. å˜‰é–çš‡å¸æ˜¯å¦æ²‰è¿·ä¸¹è¯ï¼Ÿ\n3. å¾éœå®¢å»è¿‡å“ªé‡Œï¼Ÿ\n4. åœŸæœ¨å ¡ä¹‹å˜")
    
    search_btn = st.button("å¼€å§‹åˆ†æ", type="primary")
    
    st.divider()
    st.caption("Developed by CS Year 2 Group")

# ä¸»ç•Œé¢é€»è¾‘
if search_btn or user_query:
    if not db_data:
        st.stop()
        
    # --- A. æ‰§è¡Œæœç´¢ ---
    results, query_vec = semantic_search(user_query)
    
    # å¸ƒå±€ï¼šå·¦è¾¹æ˜¾ç¤ºæ–‡æœ¬ç»“æœï¼Œå³è¾¹æ˜¾ç¤ºå¯è§†åŒ–
    col_left, col_right = st.columns([1.2, 1])
    
    # --- å·¦ä¾§ï¼šæ£€ç´¢ç»“æœ ---
    with col_left:
        st.subheader("ğŸ“– å²æ–™æ£€ç´¢ (Retrieval)")
        
        # æå–æ’åç¬¬ä¸€çš„äººåï¼Œç”¨äºæŸ¥ CBDB
        top_person_name = results[0]['data']['name']
        
        for i, res in enumerate(results):
            score = res['score']
            text = res['data']['text']
            name = res['data']['name']
            
            # åŠ¨æ€å¡ç‰‡é¢œè‰²
            border_color = "red" if i == 0 else "grey"
            
            with st.container(border=True):
                st.markdown(f"**Top {i+1} | {name}** (ç½®ä¿¡åº¦: `{score:.4f}`)")
                st.markdown(f"> {text}")

    # --- å³ä¾§ï¼šCBDB + å¯è§†åŒ– ---
    with col_right:
        # 1. CBDB æ¡£æ¡ˆå¡ç‰‡
        st.subheader("ğŸªª äººç‰©æ¡£æ¡ˆ (CBDB API)")
        
        # åªæœ‰å½“ç½®ä¿¡åº¦æ¯”è¾ƒé«˜æ—¶ï¼Œæ‰å»æŸ¥ CBDBï¼ŒèŠ‚çœ API èµ„æº
        if results[0]['score'] > 0.4:
            with st.spinner(f"æ­£åœ¨è¿æ¥å“ˆä½›æœåŠ¡å™¨æŸ¥è¯¢ {top_person_name}..."):
                bio = get_cbdb_bio(top_person_name)
            
            if bio:
                st.success(f"å·²æ‰¾åˆ° **{top_person_name}** çš„å®˜æ–¹è®°å½•")
                col_a, col_b = st.columns(2)
                col_a.metric("ç”Ÿå’å¹´", f"{bio['birth']} - {bio['death']}")
                col_a.metric("ç±è´¯", bio['native'])
                col_b.metric("CBDB ID", bio['id'])
                col_b.metric("æœä»£", bio['dynasty'])
            else:
                st.warning(f"CBDB æš‚æ—  {top_person_name} çš„ç»“æ„åŒ–æ•°æ® (æˆ–ç½‘ç»œè¶…æ—¶)")
        else:
            st.info("æœªæ£€æµ‹åˆ°æ˜ç¡®çš„å†å²äººç‰©ï¼Œæš‚ä¸è°ƒç”¨ CBDBã€‚")

        # 2. å‘é‡ç©ºé—´æ•£ç‚¹å›¾ (äº®ç‚¹!)
        st.divider()
        st.subheader("ğŸŒŒ è¯­ä¹‰ç©ºé—´å¯è§†åŒ– (PCA)")
        
        # å‡†å¤‡ç»˜å›¾æ•°æ®
        # æˆ‘ä»¬æŠŠæ•°æ®åº“é‡Œçš„å‰ 50 æ¡æ‹¿å‡ºæ¥ç”»ï¼Œå¤ªå¤šä¼šä¹±
        subset_indices = list(range(min(len(db_data), 50)))
        subset_vecs = db_embeddings[subset_indices]
        subset_names = [db_data[i]['name'] for i in subset_indices]
        subset_texts = [db_data[i]['text'][:30] for i in subset_indices]
        
        # æŠŠç”¨æˆ·çš„æŸ¥è¯¢å‘é‡ä¹ŸåŠ è¿›å»
        all_vecs = np.vstack([subset_vecs, query_vec])
        
        # PCA é™ç»´åˆ° 2D
        pca = PCA(n_components=2)
        all_coords = pca.fit_transform(all_vecs)
        
        # æ„å»º DataFrame
        df = pd.DataFrame({
            'x': all_coords[:-1, 0],
            'y': all_coords[:-1, 1],
            'name': subset_names,
            'desc': subset_texts,
            'type': ['History'] * len(subset_names)
        })
        
        # æ·»åŠ ç”¨æˆ·æŸ¥è¯¢ç‚¹
        query_df = pd.DataFrame({
            'x': [all_coords[-1, 0]],
            'y': [all_coords[-1, 1]],
            'name': ['YOUR QUERY'],
            'desc': [user_query],
            'type': ['Query']
        })
        
        final_df = pd.concat([df, query_df])
        
        # Plotly ç”»å›¾
        fig = px.scatter(final_df, x='x', y='y', color='name', symbol='type',
                         hover_data=['desc'], size_max=15, 
                         title="è¯­ä¹‰è·ç¦»åˆ†å¸ƒå›¾")
        # æ ‡è®°å‡º Query ç‚¹ä¸ºå¤§æ˜Ÿæ˜Ÿ
        fig.update_traces(marker=dict(size=12))
        
        st.plotly_chart(fig, use_container_width=True)
        st.caption("âœ¨ å›¾ä¸­è·ç¦»è¶Šè¿‘çš„ç‚¹ï¼Œè¡¨ç¤ºè¯­ä¹‰ï¼ˆå«ä¹‰ï¼‰è¶Šç›¸ä¼¼ã€‚çº¢æ˜Ÿä»£è¡¨ä½ çš„é—®é¢˜ã€‚")

else:
    st.write("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ è¾“å…¥é—®é¢˜å¹¶ç‚¹å‡»â€œå¼€å§‹åˆ†æâ€")